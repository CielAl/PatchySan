function [acc,varargout] = kfold_train(hyperparameter,graphDB,graphNetHandle,foldNum,scale,net,optsIn)
% Usage: a wrapper for k-fold cross validation.
% -- Input --
% hyperparameter: [miniBatchSize, Epoch]. Scaled by scale.
% graphDB: input graphDB generated by readGraphs.m.
% foldNum: the number of folds. If 1, then simply train the first 80% of data and test with the 20% data points after.
% scale: [s1,s2]. The real hyperparameters are [s1*miniBatchSize, s2*Epoch]. The purpose is to shrink the space of hyperparameters while the range of values can maintain the same.
% net: The net to train. It can be a pretrained DagNN network. If empty then generate a new DagNN net based on graphDB.
% optsIn: Opts for DagNN. (e.g. use GPU/flag to plot/..)
% -- Output --
% acc: Test Accuracy:
% varagout: If foldNum = 1, only returns the trained net. If performs K-CV, then varagout{1} = list of test Accuracy for all folds. varagout{2} is the net with best test accuracy.

%rewrite the trainCNN. 
%Use default values for most of unimportant (hyper)parameters.
%For grid search/random search/GA: the inner loop of cv.
if nargin < 3
	graphNetHandle = graphNet;
end

if nargin< 6 
    [net,~]=graphNetHandle(graphDB.meta);
end
if nargin < 5
    scale = [1 1];
end
if nargin< 4
    foldNum = 10;
end
if foldNum>1 && nargout<5
    error('Output for k >1: [avgAccuracy,accList,bestNet,trainList,netList]');
elseif foldNum==1 && nargout>5
     error('Output for k = 1: [accuracy,list]');
end
%% Prepare Net

graphDB.meta.sets={'train','val'};
[~,~,~,ss] = size(graphDB.data.input);
graphDB.data.set = ones(1,ss);
net.conserveMemory = true;
%% Default
trainOutput ='./data/trainResult';

%% opts

%opts.train.numSubBatches = 1 ;
opts.train.continue = false; 
opts.train.prefetch = false ;
opts.train.expDir = trainOutput ; 
opts.train.derOutputs = {'objective',1} ;
opts.train.gpus = 1;
opts.train.plotStatistics = 1;% foldNum==1;

%% Hyper-param
% Scale is the step size for hyperparameter. Usually to scaleup integers/
miniBatch = int32(hyperparameter(1)*scale(1));
epoch =  int32(hyperparameter(2)*scale(2));

opts.train.batchSize = miniBatch;
graphDB.batchSize = opts.train.batchSize ;
firstPhase = floor(epoch*0.5);
secondPhase = epoch - firstPhase;
opts.train.learningRate = logspace(-1,-5,epoch);%[5e-5*ones(1,5)  1e-5*ones(1,max(0,firstPhase-5)) 5e-6*ones(1,secondPhase)];
%opts.train.learningRate = [1e-4*ones(1,5)  5e-5*ones(1,max(0,firstPhase-5)) 1e-5*ones(1,secondPhase)];
opts.train.weightDecay =0;%0.0005;
opts.train.numEpochs = numel(opts.train.learningRate) ;
opts.train.derOutputs = {'objective',1} ;
opts.train.gpus = 1;


%% Compile opts
 [opts, ~] = vl_argparse(opts.train, {'gpu',1}) ;
 if nargin>6
   [opts, ~] = vl_argparse(opts.train, optsIn) ;   
 end
 %% Prepare Folds.
[~,~,~,maxSize]= size(graphDB.data.input);
foldSize = floor(maxSize/foldNum);
acc_accumulate = 0;
%{
% already shuffled. Disabled due to it might be called in inner loops of
tuning
randIndex = randperm(maxSize);
graphDB.data.input= graphDB.data.input(:,:,:,randIndex); 
graphDB.data.label= graphDB.data.label(:,:,:,randIndex); 
%}
if foldNum==1
    % the latter 20% are for test
    graphDB.data.set((floor(ss*0.8)+1):end) = 2;
    [net,~] = cnn_train_dag(net, graphDB, @getBatch,opts) ;
    net.move('cpu');
    gpuDevice;
    net.move('gpu');
    acc = kfold_test(net,graphDB,1,1);
    varargout{1} = net;
    varargout{2} = [];
    varargout{3} = [];
    varargout{4} = [];
    
    return;
end
accList   = zeros(1,foldNum);
trainList =  zeros(1,foldNum);
accMax = 0;
netList = cell(1,foldNum);
    for ii=1:foldNum
        fprintf('Fold#:%d\n',ii);
        foldHead  = (ii-1)*foldSize+1;
        foldTail = min(foldHead + foldSize -1,maxSize);
        graphDB.data.set( foldHead:foldTail) = 2;       
        [net,~] = cnn_train_dag(net, graphDB, @getBatch,opts) ;
        accList(ii) = kfold_test(net,graphDB,1);
        trainList(ii) = kfold_test(net,graphDB,1,2);
        fprintf(' Accuracy#:%f\n',accList(ii));
          fprintf(' Train#:%f\n',trainList(ii));
         net.move('cpu');
         netList{ii} = net;
        if (accList(ii)>accMax)
            accMax = accList(ii);
            netBest = net;
        end
        %re-init the net for the next round
           
           
                graphDB.data.set = ones(1,ss);
                [net] =  graphNetHandle(graphDB.meta);       
                
    end
     varargout{1} = accList;
    acc =sum(accList)/foldNum;
     varargout{2} = netBest;
        varargout{3} = trainList;
        varargout{4} = netList;
end
